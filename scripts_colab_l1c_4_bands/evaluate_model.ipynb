{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b740d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "INICIANDO A AVALIAÇÃO DO MODELO\n",
      "-> Checkpoint: C:\\Users\\alanb\\Documents\\cloud-detection-dissertation\\Unet_4_bands_l1c\\lightning_logs\\Unet_efficientnet-b1\\epoch=43-train_loss=0.25-val_loss=0.26-trainHigh512.ckpt\n",
      "==================================================\n",
      "1. Carregando modelo e datamodule...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alanb\\anaconda3\\envs\\cd\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:627: UserWarning: This DataLoader will create 11 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Realizando inferência no conjunto de teste...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avaliando: 100%|██████████| 975/975 [13:48<00:00,  1.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Consolidando predições e preparando para análise...\n",
      "4. Calculando métricas com Scikit-learn...\n",
      "\n",
      "======================================================\n",
      "        RELATÓRIO DE AVALIAÇÃO FINAL\n",
      "======================================================\n",
      "Checkpoint: epoch=43-train_loss=0.25-val_loss=0.26-trainHigh512.ckpt\n",
      "\n",
      "MÉTRICAS GERAIS:\n",
      "--------------------------------\n",
      "Acurácia Geral: 0.8972\n",
      "IoU (Macro):    0.7195\n",
      "IoU (Ponderado):0.8198\n",
      "\n",
      "RELATÓRIO DETALHADO POR CLASSE:\n",
      "--------------------------------\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Ceu Limpo     0.9334    0.9476    0.9404 136267815\n",
      "  Nuvem Espessa     0.9064    0.9306    0.9183  73075814\n",
      "     Nuvem Fina     0.7188    0.6369    0.6754  21985068\n",
      "Sombra de Nuvem     0.8002    0.7498    0.7741  24261703\n",
      "\n",
      "       accuracy                         0.8972 255590400\n",
      "      macro avg     0.8397    0.8162    0.8271 255590400\n",
      "   weighted avg     0.8946    0.8972    0.8955 255590400\n",
      "\n",
      "\n",
      "5. Gerando matriz de confusão...\n",
      "\n",
      "Relatório de classificação salvo em: C:\\Users\\alanb\\Documents\\cloud-detection-dissertation\\Unet_4_bands_l1c\\lightning_logs\\Unet_efficientnet-b1\\output\\report_epoch=43-train_loss=0.25-val_loss=0.26-trainHigh512.txt\n",
      "Matriz de confusão salva em: C:\\Users\\alanb\\Documents\\cloud-detection-dissertation\\Unet_4_bands_l1c\\lightning_logs\\Unet_efficientnet-b1\\output\\confusion_matrix_epoch=43-train_loss=0.25-val_loss=0.26-trainHigh512.png\n",
      "\n",
      "Análise concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import tacoreader\n",
    "\n",
    "# Importa as suas classes customizadas\n",
    "from model import UNet_CD_Sentinel_2\n",
    "from datamodule import CoreDataModule # Certifique-se que o nome do arquivo está correto\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, jaccard_score, accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, output_path):\n",
    "    \"\"\"\n",
    "    Renderiza e salva a matriz de confusão como uma imagem.\n",
    "    A matriz é normalizada para mostrar percentuais (revocação por classe).\n",
    "    \"\"\"\n",
    "    # Normaliza a matriz de confusão pela linha (representa a revocação de cada classe)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm_normalized, \n",
    "        annot=True, \n",
    "        fmt=\".2%\",  # Formato de porcentagem com 2 casas decimais\n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title(\"Matriz de Confusão Normalizada (Revocação por Classe)\")\n",
    "    plt.ylabel('Classe Verdadeira (Ground Truth)')\n",
    "    plt.xlabel('Classe Predita pelo Modelo')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Matriz de confusão salva em: {output_path}\")\n",
    "\n",
    "def main(checkpoint_path, df, class_names, output_dir, batch_size):\n",
    "    \"\"\"\n",
    "    Função principal para carregar o modelo e avaliar as métricas.\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"INICIANDO A AVALIAÇÃO DO MODELO\")\n",
    "    print(f\"-> Checkpoint: {checkpoint_path}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- 1. Carregar Modelo e Dados ---\n",
    "    print(\"1. Carregando modelo e datamodule...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Carrega o modelo a partir do checkpoint\n",
    "    model = UNet_CD_Sentinel_2.load_from_checkpoint(checkpoint_path).to(device)\n",
    "    model.eval() # Coloca o modelo em modo de avaliação\n",
    "    \n",
    "    # Assume que o datamodule está configurado com os mesmos dados do treinamento\n",
    "    # Você pode precisar ajustar os argumentos aqui se o seu datamodule precisar\n",
    "    datamodule = CoreDataModule(\n",
    "        dataframe=df, # O dataframe será carregado dentro do setup\n",
    "        batch_size=batch_size\n",
    "        )\n",
    "    datamodule.setup('test') # Configura o conjunto de teste\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    \n",
    "    # --- 2. Realizar Inferência ---\n",
    "    print(\"2. Realizando inferência no conjunto de teste...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad(): # Desativa o cálculo de gradientes para acelerar\n",
    "        for batch in tqdm(test_loader, desc=\"Avaliando\"):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            upsampled_logits = F.interpolate(\n",
    "                logits,\n",
    "                size=labels.shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "            preds = torch.argmax(upsampled_logits, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # --- 3. Preparar Dados para Scikit-learn ---\n",
    "    print(\"3. Consolidando predições e preparando para análise...\")\n",
    "    # Concatena os resultados de todos os lotes em um único tensor\n",
    "    preds_tensor = torch.cat(all_preds)\n",
    "    labels_tensor = torch.cat(all_labels)\n",
    "    \n",
    "    # Converte para NumPy e achata para um vetor 1D\n",
    "    preds_np = preds_tensor.numpy().flatten()\n",
    "    labels_np = labels_tensor.numpy().flatten()\n",
    "\n",
    "    # --- 4. Calcular Métricas e Gerar Relatórios ---\n",
    "    print(\"4. Calculando métricas com Scikit-learn...\")\n",
    "\n",
    "    # Gera o relatório de classificação detalhado\n",
    "    report = classification_report(\n",
    "        labels_np, \n",
    "        preds_np, \n",
    "        target_names=class_names, \n",
    "        digits=4\n",
    "    )\n",
    "\n",
    "    # Calcula métricas gerais\n",
    "    overall_accuracy = accuracy_score(labels_np, preds_np)\n",
    "    iou_macro = jaccard_score(labels_np, preds_np, average='macro')\n",
    "    iou_weighted = jaccard_score(labels_np, preds_np, average='weighted')\n",
    "\n",
    "    # Monta o relatório final\n",
    "    final_report = f\"\"\"\n",
    "======================================================\n",
    "        RELATÓRIO DE AVALIAÇÃO FINAL\n",
    "======================================================\n",
    "Checkpoint: {os.path.basename(checkpoint_path)}\n",
    "\n",
    "MÉTRICAS GERAIS:\n",
    "--------------------------------\n",
    "Acurácia Geral: {overall_accuracy:.4f}\n",
    "IoU (Macro):    {iou_macro:.4f}\n",
    "IoU (Ponderado):{iou_weighted:.4f}\n",
    "\n",
    "RELATÓRIO DETALHADO POR CLASSE:\n",
    "--------------------------------\n",
    "{report}\n",
    "\"\"\"\n",
    "    print(final_report)\n",
    "\n",
    "    # --- 5. Gerar Matriz de Confusão ---\n",
    "    print(\"5. Gerando matriz de confusão...\")\n",
    "    cm = confusion_matrix(labels_np, preds_np)\n",
    "    \n",
    "    # Cria o diretório de saída se ele não existir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define os nomes dos arquivos de saída\n",
    "    base_name = os.path.splitext(os.path.basename(checkpoint_path))[0]\n",
    "    report_path = os.path.join(output_dir, f\"report_{base_name}.txt\")\n",
    "    cm_path = os.path.join(output_dir, f\"confusion_matrix_{base_name}.png\")\n",
    "\n",
    "    # --- 6. Salvar Resultados ---\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(final_report)\n",
    "    print(f\"\\nRelatório de classificação salvo em: {report_path}\")\n",
    "\n",
    "    plot_confusion_matrix(cm, class_names, cm_path)\n",
    "\n",
    "    print(\"\\nAnálise concluída com sucesso!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = tacoreader.load([ r\"D:\\taco_CloudSen12\\cloudsen12-l1c.0000.part.taco\",\n",
    "                                r\"D:\\taco_CloudSen12\\cloudsen12-l1c.0001.part.taco\",\n",
    "                                r\"D:\\taco_CloudSen12\\cloudsen12-l1c.0002.part.taco\",\n",
    "                                r\"D:\\taco_CloudSen12\\cloudsen12-l1c.0003.part.taco\",\n",
    "                                r\"D:\\taco_CloudSen12\\cloudsen12-l1c.0004.part.taco\",\n",
    "                                ])\n",
    "    df = dataset[(dataset[\"label_type\"] == \"high\") & (dataset[\"real_proj_shape\"] == 509)]\n",
    "    \n",
    "    \n",
    "    checkpoint_path = r\"C:\\Users\\alanb\\Documents\\cloud-detection-dissertation\\Unet_4_bands_l1c\\lightning_logs\\Unet_efficientnet-b1\\epoch=43-train_loss=0.25-val_loss=0.26-trainHigh512.ckpt\"\n",
    "    class_names = [\"Ceu Limpo\", \"Nuvem Espessa\", \"Nuvem Fina\", \"Sombra de Nuvem\"]\n",
    "    output_dir = r\"C:\\Users\\alanb\\Documents\\cloud-detection-dissertation\\Unet_4_bands_l1c\\lightning_logs\\Unet_efficientnet-b1\\output\"\n",
    "    batch_size = 1\n",
    "    \n",
    "    main(checkpoint_path, df, class_names, output_dir, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
